{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc0ec97-4a22-4144-934c-581faf739234",
   "metadata": {},
   "source": [
    "## Python Lab Activity 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9ecdbf-6963-4595-8bcd-0e81caf123a3",
   "metadata": {},
   "source": [
    "## Factorial of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03ab9ff6-941d-4a0c-bc9d-b1cece53109e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "num = 100\n",
    "fact = 1\n",
    "while num > 0:\n",
    "    fact = fact * num\n",
    "    num = num - 1\n",
    "\n",
    "print(fact)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae24bd9a-fc73-4e18-a990-85c3bad5e888",
   "metadata": {},
   "source": [
    "## String is Immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34fd4bdf-6d93-4ed4-ab72-658bd166159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133646500650688\n",
      "133646500654528\n",
      "133646500648480\n",
      "133646500650688\n",
      "133646500654528\n",
      "133646500648480\n",
      "133646500650688\n",
      "133646245455280\n",
      "133646234575344\n",
      "133646245455280\n",
      "0123456789\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\n",
    "for i in range(10):\n",
    "    s += str(i)\n",
    "    print(id(s))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bfa69c2-084a-4f01-9f06-2df671d3ce56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133645498693552\n",
      "0123456789\n"
     ]
    }
   ],
   "source": [
    "#  Alternative to save memory space\n",
    "char_list = []\n",
    "for i in range(10):\n",
    "    char_list.append(str(i))\n",
    "s = \"\".join(char_list)\n",
    "print(id(s))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d0ca4-97fd-404d-b576-ac69a779d646",
   "metadata": {},
   "source": [
    "## Handling Character not in keyboard using UNICODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40d47e83-e0f1-4e11-a157-0bb6ebab8a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î©\n"
     ]
    }
   ],
   "source": [
    "print('\\u03A9')  # using unicode representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "317b8fc6-92f7-4fb2-b45c-e6d6b152d451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0001F600\n"
     ]
    }
   ],
   "source": [
    "print('\\u0001F600')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e406a572-a287-4c48-97be-29d96d743c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata  # using inbuilt library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a8f3701-a705-41ba-ad72-a021e30d3b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = unicodedata.lookup('GREEK CAPITAL LETTER OMEGA')\n",
    "smiley = unicodedata.lookup('GRINNING FACE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91496029-e4b5-4311-b902-7176270ec26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ðŸ˜€'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiley  # Directly using keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d15ed620-b324-4d3f-8c62-b7841397ae39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Î©'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dfc47b-7032-4234-8211-99eb07987ddf",
   "metadata": {},
   "source": [
    "# Encoding equivalent of Keyboard character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf8754b7-f0f2-4106-93b6-476e28760e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the character from keyboard:  A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicode value of the character is  \\u0041\n"
     ]
    }
   ],
   "source": [
    "character = input(\"Enter the character from keyboard: \")\n",
    "unicode = ord(character)\n",
    "unicode_seq = f'\\\\u{unicode:04x}'\n",
    "print(\"Unicode value of the character is \",unicode_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d833af5a-467d-473a-9f03-278072b666a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicode value of the character is  \\u4f60\n"
     ]
    }
   ],
   "source": [
    "# Encoding Chinese character\n",
    "character = 'ä½ '   # character = 'H'\n",
    "unicode = ord(character)\n",
    "unicode_seq = f'\\\\u{unicode:04x}'\n",
    "print(\"Unicode value of the character is \",unicode_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b99cf1a0-fd53-407b-9fa5-fdedf63468f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicode value of the character is  \\u0ca4\n"
     ]
    }
   ],
   "source": [
    "# Encoding undocumented character (Tulu language)\n",
    "character = 'à²¤'   # character 'T'\n",
    "unicode = ord(character)\n",
    "unicode_seq = f'\\\\u{unicode:04x}'\n",
    "print(\"Unicode value of the character is \",unicode_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdba718-e7aa-43c3-9bef-49be40f8d645",
   "metadata": {},
   "source": [
    "## Natural Language Toolkit (NLTK) library to perform Part-of-Speech (POS) tagging on a given text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2054a88a-6201-415b-b4dc-d87ad70ed868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c15a7b3d-2c7a-4a21-ad1f-88c3ab1d6ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package english_wordnet to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/english_wordnet.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package mock_corpus to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mock_corpus.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data]    | Downloading package qc to /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /home/matlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1a297d3-c3b9-47bf-82b9-e60b9a33fef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('T', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('s', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('a', 'DT')]\n",
      "[('l', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('W', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('.', '.')]\n",
      "[('I', 'PRP')]\n",
      "[('t', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('M', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('S', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('r', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('s', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('.', '.')]\n",
      "[('S', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('v', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('d', 'NN')]\n",
      "[(',', ',')]\n",
      "[('t', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('.', '.')]\n",
      "[('W', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('s', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('p', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('.', '.')]\n",
      "[('N', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('m', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('l', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('v', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('i', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('b', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('.', '.')]\n",
      "[('I', 'PRP')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('d', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('u', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('c', 'NNS')]\n",
      "[('k', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[(',', ',')]\n",
      "[('b', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('l', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('o', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('t', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('k', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('s', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('v', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[(',', ',')]\n",
      "[('l', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('k', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('h', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('k', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "texts = '''The dog was called Wellington. It belonged to Mrs Shears who was our friend. She lived on the opposite side of the road, two houses to the left.\n",
    "Wellington was a poodle. Not one of the small poodles that have hairstyles but a big poodle. It had curly black fur, but when you got close you could see that the skin underneath the fur was a very pale yellow, like chicken.\n",
    "'''\n",
    "for text in texts:\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        tagged_words = nltk.pos_tag(words)\n",
    "        print(tagged_words)   # This line prints the list of tuples, where each tuple contains a word and its corresponding POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a0464cc-d5ba-44c6-9245-c961b272b1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags for the first 5 sentences:\n",
      "Sentence 1: [('\\ufeffThe', 'NN'), ('Project', 'NNP'), ('Gutenberg', 'NNP'), ('eBook', 'NN'), ('of', 'IN'), ('The', 'DT'), ('Scarlet', 'NNP'), ('Letter', 'NNP'), ('This', 'DT'), ('ebook', 'NN'), ('is', 'VBZ'), ('for', 'IN'), ('the', 'DT'), ('use', 'NN'), ('of', 'IN'), ('anyone', 'NN'), ('anywhere', 'RB'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('and', 'CC'), ('most', 'JJS'), ('other', 'JJ'), ('parts', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('world', 'NN'), ('at', 'IN'), ('no', 'DT'), ('cost', 'NN'), ('and', 'CC'), ('with', 'IN'), ('almost', 'RB'), ('no', 'DT'), ('restrictions', 'NNS'), ('whatsoever', 'RB'), ('.', '.')]\n",
      "Sentence 2: [('You', 'PRP'), ('may', 'MD'), ('copy', 'VB'), ('it', 'PRP'), (',', ','), ('give', 'VB'), ('it', 'PRP'), ('away', 'RB'), ('or', 'CC'), ('re-use', 'VB'), ('it', 'PRP'), ('under', 'IN'), ('the', 'DT'), ('terms', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Project', 'NNP'), ('Gutenberg', 'NNP'), ('License', 'NNP'), ('included', 'VBD'), ('with', 'IN'), ('this', 'DT'), ('ebook', 'NN'), ('or', 'CC'), ('online', 'NN'), ('at', 'IN'), ('www.gutenberg.org', 'NN'), ('.', '.')]\n",
      "Sentence 3: [('If', 'IN'), ('you', 'PRP'), ('are', 'VBP'), ('not', 'RB'), ('located', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), (',', ','), ('you', 'PRP'), ('will', 'MD'), ('have', 'VB'), ('to', 'TO'), ('check', 'VB'), ('the', 'DT'), ('laws', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('country', 'NN'), ('where', 'WRB'), ('you', 'PRP'), ('are', 'VBP'), ('located', 'VBN'), ('before', 'IN'), ('using', 'VBG'), ('this', 'DT'), ('eBook', 'NN'), ('.', '.')]\n",
      "Sentence 4: [('Title', 'NN'), (':', ':'), ('The', 'DT'), ('Scarlet', 'NNP'), ('Letter', 'NNP'), ('Author', 'NNP'), (':', ':'), ('Nathaniel', 'NNP'), ('Hawthorne', 'NNP'), ('Engraver', 'NNP'), (':', ':'), ('A.', 'NN'), ('V.', 'NNP'), ('S.', 'NNP'), ('Anthony', 'NNP'), ('Illustrator', 'NNP'), (':', ':'), ('Mary', 'NNP'), ('Hallock', 'NNP'), ('Foote', 'NNP'), ('Ludvig', 'NNP'), ('SandÃ¶e', 'NNP'), ('Ipsen', 'NNP'), ('Release', 'NNP'), ('date', 'NN'), (':', ':'), ('May', 'NNP'), ('5', 'CD'), (',', ','), ('2008', 'CD'), ('[', 'NNP'), ('eBook', 'VBD'), ('#', '#'), ('25344', 'CD'), (']', 'NN'), ('Most', 'RBS'), ('recently', 'RB'), ('updated', 'VBN'), (':', ':'), ('July', 'NNP'), ('15', 'CD'), (',', ','), ('2025', 'CD'), ('Language', 'NN'), (':', ':'), ('English', 'JJ'), ('Credits', 'NNS'), (':', ':'), ('Markus', 'NNP'), ('Brenner', 'NNP'), (',', ','), ('Irma', 'NNP'), ('Spehar', 'NNP'), ('and', 'CC'), ('the', 'DT'), ('Online', 'NNP'), ('Distributed', 'NNP'), ('Proofreading', 'NNP'), ('Team', 'NNP'), ('*', 'NNP'), ('*', 'NNP'), ('*', 'NNP'), ('START', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('PROJECT', 'NNP'), ('GUTENBERG', 'NNP'), ('EBOOK', 'NNP'), ('THE', 'NNP'), ('SCARLET', 'NNP'), ('LETTER', 'NNP'), ('*', 'NNP'), ('*', 'NNP'), ('*', 'VBD'), ('THE', 'NNP'), ('SCARLET', 'NNP'), ('LETTER', 'NNP'), ('.', '.')]\n",
      "Sentence 5: [('BY', 'NNP'), ('NATHANIEL', 'NNP'), ('HAWTHORNE', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from urllib import request\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/25344/pg25344.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf-8')\n",
    "\n",
    "# It's a good practice to remove the Project Gutenberg header/footer\n",
    "# This part is specific to the structure of Project Gutenberg texts\n",
    "start_of_book = \"*** START OF THE PROJECT GUTENBERG EBOOK CRIME AND PUNISHMENT ***\"\n",
    "end_of_book = \"*** END OF THE PROJECT GUTENBERG EBOOK CRIME AND PUNISHMENT ***\"\n",
    "\n",
    "# Find the start and end of the actual novel content\n",
    "start_index = raw.find(start_of_book)\n",
    "end_index = raw.find(end_of_book)\n",
    "\n",
    "# Extract the novel content\n",
    "if start_index != -1 and end_index != -1:\n",
    "    novel_text = raw[start_index + len(start_of_book):end_index].strip()\n",
    "elif start_index != -1:  # If only start marker is found\n",
    "    novel_text = raw[start_index + len(start_of_book):].strip()\n",
    "else:  # If no markers found, use the raw text (less accurate)\n",
    "    novel_text = raw\n",
    "\n",
    "# Tokenize the novel into sentences\n",
    "sentences = nltk.sent_tokenize(novel_text)\n",
    "\n",
    "# Process and print POS tags for each sentence\n",
    "# For brevity, let's process and print only the first few sentences\n",
    "print(\"POS Tags for the first 5 sentences:\")\n",
    "for i, sentence in enumerate(sentences[:5]): # Process only the first 5 sentences\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    print(f\"Sentence {i+1}: {tagged_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293466cb-d82b-4388-85ea-1e9f56b12a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
